{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PCA-SVD.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.2"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yandexdataschool/MLatImperial2021/blob/master/03_lab/PCA_SVD.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HjlDIeY5LI0G"
      },
      "source": [
        "import scipy as sp\n",
        "import numpy as np\n",
        "import scipy.linalg as sla\n",
        "import scipy.sparse as sps\n",
        "import scipy.sparse.linalg as spla\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "%pylab inline\n",
        "\n",
        "import pandas as pd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3TZfXzvQLI0R"
      },
      "source": [
        "# PCA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sRuHSWS9LI0S"
      },
      "source": [
        "We have a object - feature matrix $F$ of size l x n,\n",
        "\n",
        "For the PCA the main task is to find such weight matrix $W$ such that \n",
        "\n",
        "$$\n",
        "G = FW\n",
        "$$\n",
        "\n",
        "and each column-vector of G is a principal component, that maximises the residual variance of the data.\n",
        "\n",
        "$$\n",
        "argmax_w \\sum_{i=1}^l G^2_{i1} = argmax_w \\sum_{i=1}^l (FW)^2_{ij}\n",
        "\n",
        "$$\n",
        "\n",
        "where $G$ - matrix of principle components of size l x m, $W$ is transformation matrix of size n x m from old features to new.\n",
        "\n",
        "Why do we maximis this objective?\n",
        "\n",
        "Hint:\n",
        "\n",
        "$$\n",
        "Var(Z) = E[(Z - E[Z])^2]\n",
        "$$\n",
        "\n",
        "\n",
        "\n",
        "Columns of matrix $W$ represent principal axis in the feature space.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LaKPOVHjLI0T"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "random_state = 0\n",
        "\n",
        "# Load Digits dataset\n",
        "X, y = datasets.load_digits(return_X_y=True)\n",
        "\n",
        "# Split into train/test\n",
        "X_train, X_test, y_train, y_test = \\\n",
        "    train_test_split(X, y, test_size=0.5, stratify=y,\n",
        "                     random_state=random_state)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E9b5ddE6LI0Z"
      },
      "source": [
        "Here we will study the something like mnist dataset - images of numbers from 0 to 9. each of which is size 8x8 pixels"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pAUYhrz4LI0b"
      },
      "source": [
        "X.shape, y.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "le9__yimLI0g"
      },
      "source": [
        "from mpl_toolkits.axes_grid1 import ImageGrid\n",
        "fig = plt.figure(1,(10,10))\n",
        "grid = ImageGrid(fig, 111,\n",
        "                 nrows_ncols=(2,7),\n",
        "                 axes_pad=0.1,\n",
        "                 )\n",
        "for i in range(14):\n",
        "    image = X[i,:].squeeze().reshape(8,8)\n",
        "    grid[i].imshow(image,cmap='gray',interpolation='none')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gKL9a1EgLI0l"
      },
      "source": [
        "We can combine all the data preporcessing and algorithm we want to fit with sklearn make_pipeline tool. Thats comes very handy once you want to write more maintainable code and will be easy to check for bugs and change."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bvOzdXoZLI0m"
      },
      "source": [
        "dim = len(X[0])\n",
        "n_classes = len(np.unique(y))\n",
        "\n",
        "n_neighbors = 3\n",
        "n_components = 2\n",
        "\n",
        "# Reduce dimension to 2 with PCA\n",
        "\n",
        "\n",
        "knn_pca = make_pipeline(StandardScaler(),\n",
        "                    PCA(n_components=n_components, random_state=1543),\n",
        "                    KNeighborsClassifier(n_neighbors=n_neighbors))\n",
        "\n",
        "# Fit the method's model\n",
        "knn_pca.fit(X_train, y_train)\n",
        "\n",
        "acc_knn = knn_pca.score(X_test, y_test)\n",
        "\n",
        "# Embed the data set in 2 dimensions using the fitted model\n",
        "X_transformed = knn_pca[:-1].transform(X)\n",
        "\n",
        "# Plot the projected points and show the evaluation score\n",
        "plt.figure()\n",
        "plt.scatter(X_transformed[:, 0], X_transformed[:, 1], c=y, s=30, cmap='Set1')\n",
        "plt.title(\"{}, KNN (k={})\\nTest accuracy = {:.2f}\".format(\"KNN\",\n",
        "                                                          n_neighbors,\n",
        "                                                          acc_knn))\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "igOLS6BJQu0y"
      },
      "source": [
        "def calculate_score(n_neighbors, n_components):\n",
        "    ### In this function, implement fitting a pipeline\n",
        "    ### with a given number or neighbors and pca components\n",
        "    ### on the train data\n",
        "    ### and evaluating it on the test data.\n",
        "    \n",
        "    knn_pca = make_pipeline(StandardScaler(),\n",
        "                    PCA(n_components=n_components, random_state=1543),\n",
        "                    KNeighborsClassifier(n_neighbors=n_neighbors))\n",
        "\n",
        "    ### Return the test score\n",
        "    # Wait by where is X_train? and X_test? will this function run?\n",
        "    knn_pca.fit(X_train, y_train)\n",
        "    return knn_pca.score(X_test, y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "76gO_VlGLI0u"
      },
      "source": [
        "plot the dependence of the score on the n_neigbours and n_components"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hdABNYL5LI01"
      },
      "source": [
        "results = []\n",
        "\n",
        "neighbors = range(1, 21)\n",
        "components = range(1, 16)\n",
        "\n",
        "for n_n in neighbors:\n",
        "    for n_c in components:\n",
        "        results.append(calculate_score(n_n, n_c))\n",
        "\n",
        "x_pos, y_pos = np.meshgrid(components, neighbors)\n",
        "plt.figure(figsize=(12,6))\n",
        "plt.contourf(x_pos, y_pos, np.array(results).reshape(x_pos.shape), levels=100);\n",
        "plt.colorbar()\n",
        "plt.xlabel(\"N_components\",fontsize=19)\n",
        "plt.ylabel(\"N_neigbours\",fontsize=19);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2-gjugdiLI05"
      },
      "source": [
        "### Lets take another dataset of wines and see the effect of the data standatisation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rUN58Mb3LI06"
      },
      "source": [
        "from __future__ import print_function\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn import metrics\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.pipeline import make_pipeline\n",
        "print(__doc__)\n",
        "\n",
        "# Code source: Tyler Lanigan <tylerlanigan@gmail.com>\n",
        "#              Sebastian Raschka <mail@sebastianraschka.com>\n",
        "\n",
        "# License: BSD 3 clause\n",
        "\n",
        "RANDOM_STATE = 42\n",
        "FIG_SIZE = (10, 7)\n",
        "\n",
        "features = pd.DataFrame(load_wine(return_X_y=False)['data'],\n",
        "                        columns=load_wine(return_X_y=False)['feature_names'])\n",
        "\n",
        "target = load_wine(return_X_y=False)['target']\n",
        "features.head(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aaxC9MNRLI09"
      },
      "source": [
        "# Make a train/test split using 30% test size\n",
        "X_train, X_test, y_train, y_test = train_test_split(features, target,\n",
        "                                                    test_size=0.30,\n",
        "                                                    random_state=RANDOM_STATE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ROT3SZP6LI1E"
      },
      "source": [
        "OK, now when you now how to make pipeline, make pipeline with standard scaler and PCA and just PCA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iILFnpOaLI1F"
      },
      "source": [
        "# Fit to data and predict using pipelined PCA.\n",
        "# Fit to data and predict using pipelined PCA.\n",
        "unscaled_clf = make_pipeline(PCA(n_components=None))\n",
        "unscaled_clf.fit(X_train, y_train)\n",
        "\n",
        "# Fit to data and predict using pipelined scaling, PCA.\n",
        "std_clf = make_pipeline(StandardScaler(), PCA(n_components=None))\n",
        "std_clf.fit(X_train, y_train)\n",
        "\n",
        "\n",
        "# Extract PCA from pipeline\n",
        "pca = unscaled_clf.named_steps['pca']\n",
        "pca_std = std_clf.named_steps['pca']\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hJsxcJP-LI1M"
      },
      "source": [
        "# Show first principal components\n",
        "print('\\nPC 1 without scaling:\\n', pca.components_[0])\n",
        "print('\\nPC 1 with scaling:\\n', pca_std.components_[0])\n",
        "\n",
        "# Use PCA without and with scale on X_train data for visualization.\n",
        "X_train_transformed = pca.transform(X_train)\n",
        "scaler = std_clf.named_steps['standardscaler']\n",
        "X_train_std_transformed = pca_std.transform(scaler.transform(X_train))\n",
        "\n",
        "# visualize standardized vs. untouched dataset with PCA performed\n",
        "fig, (ax1, ax2) = plt.subplots(ncols=2, figsize=FIG_SIZE)\n",
        "\n",
        "\n",
        "for l, c, m in zip(range(0, 3), ('blue', 'red', 'green'), ('^', 's', 'o')):\n",
        "    ax1.scatter(X_train_transformed[y_train == l, 0],\n",
        "                X_train_transformed[y_train == l, 1],\n",
        "                color=c,\n",
        "                label='class %s' % l,\n",
        "                alpha=0.5,\n",
        "                marker=m\n",
        "                )\n",
        "\n",
        "for l, c, m in zip(range(0, 3), ('blue', 'red', 'green'), ('^', 's', 'o')):\n",
        "    ax2.scatter(X_train_std_transformed[y_train == l, 0],\n",
        "                X_train_std_transformed[y_train == l, 1],\n",
        "                color=c,\n",
        "                label='class %s' % l,\n",
        "                alpha=0.5,\n",
        "                marker=m\n",
        "                )\n",
        "\n",
        "ax1.set_title('Training dataset after PCA')\n",
        "ax2.set_title('Standardized training dataset after PCA')\n",
        "\n",
        "for ax in (ax1, ax2):\n",
        "    ax.set_xlabel('1st principal component')\n",
        "    ax.set_ylabel('2nd principal component')\n",
        "    ax.legend(loc='upper right')\n",
        "    ax.grid()\n",
        "\n",
        "plt.tight_layout()\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VZerU_esLI1P"
      },
      "source": [
        "### Plot the variance ratio explained vs number components. Use the availible PCA class methods to do that."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YT91488zkwpE"
      },
      "source": [
        "plt.figure(figsize=(12,6))\n",
        "plt.plot(range(pca_std.n_components_), pca_std.explained_variance_ratio_)\n",
        "plt.ylabel(\"vairance ratio\")\n",
        "plt.xlabel(\"# components\")\n",
        "plt.grid()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v-NC2y0mLI1S"
      },
      "source": [
        "# SVD decomposition"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SnEzA4SnLI1T"
      },
      "source": [
        "if M is m x n matrix over field K, there is exists factorisation of it:\n",
        "\n",
        "$$\n",
        "M = U * S * V^{\\dagger}, where\n",
        "$$\n",
        "- $U$ - is m x m unitary matrix over K,\n",
        "- $S$ - is diagonal m x n matrix with non-negative real numbers,\n",
        "- $V$ - is n x n unitary matrix over K.\n",
        "\n",
        "The values $s_i$ of matrix S are known as singular values of M.\n",
        "This decomposition is called Singular Value Decomposition - SVD.\n",
        "\n",
        "Columns of $U$ anv $V$ are called left and right singular vectors of $M$:\n",
        "$$\n",
        "M v = s u, \\\n",
        "M^{\\dagger} u = sv\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XBECbdmcLI1U"
      },
      "source": [
        "Various application in mathematics and optimisation - pseudo-inverse computation, low rank factorisation, application in solving systems of equations ..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m4XuSMMULI1U"
      },
      "source": [
        "If we define matrix $M$ to be $F$, and \n",
        "\n",
        "$$\n",
        "G = U * S,\n",
        "$$\n",
        "\n",
        "we will get full PCA decomposition, where weight matrix $W$ is now $V$.\n",
        "\n",
        "So, to get first K principal components we will just take first K columns of matrix $S * U$.\n",
        "\n",
        "#### We can also look at those components in the initial basis of M. To do that we multiply them to the firt K rows of matrix $V^{\\dagger}$.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K8Tzz2usTun6"
      },
      "source": [
        "!wget https://github.com/yandexdataschool/MLatImperial2021/raw/master/03_lab/swisscows_edit.jpg"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vXPFmqzlLI1V"
      },
      "source": [
        "from PIL import Image\n",
        "from matplotlib.pyplot import imread\n",
        "from skimage import color\n",
        "\n",
        "\n",
        "img = color.rgb2grey(imread(r'swisscows_edit.jpg'))\n",
        "img.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uz3vakIkLI1Y"
      },
      "source": [
        "imgplot = plt.imshow(img, cmap='Greys_r')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8bPaagTlLI1d"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o6Dbcy0uLI1g"
      },
      "source": [
        "# PCA via SVD for compression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "09Yh-EWWLI1h"
      },
      "source": [
        "We will use svd from scipy package"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mj1t9M_fLI1i"
      },
      "source": [
        "U, s, V_h = sla.svd(img, full_matrices=False)\n",
        "print(U.shape, s.shape, V_h.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wPTuW36RLI1k"
      },
      "source": [
        "U, s, V_h = sla.svd(img, full_matrices=False)\n",
        "pca_1 = (U[:,0]*s[0])[:,np.newaxis].dot(V_h[0,:][np.newaxis,:])\n",
        "pca_1.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g2gI8F_DX-kd"
      },
      "source": [
        "plt.imshow(pca_1, cmap='Greys_r');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ndTDTDJWLI1n"
      },
      "source": [
        "#### Now write a function that will return pricipal components from Ith to Jth in intial basis (Hint: look how we have calculated the first component in the initial basis)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yQsNuLk_LI1o"
      },
      "source": [
        "U, s, V_h = sla.svd(img, full_matrices=False)\n",
        "def PCA(start_component = 0, end_component = 1, U = U, s = s, V_h = V_h):\n",
        "    num_of_cols = end_component - start_component\n",
        "    US = (U[:,start_component:end_component]*s[start_component:end_component]).reshape(U.shape[0],num_of_cols)\n",
        "    return US.dot(V_h[start_component:end_component,:].reshape(num_of_cols,V_h.shape[1]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w0ybM2w4LI1t"
      },
      "source": [
        "pca_1 = PCA()\n",
        "pca_1_20 = PCA(end_component=20)\n",
        "pca_1_50 = PCA(end_component=50)\n",
        "pca_20_100 = PCA(20, 100)\n",
        "pca_20_end = PCA(20, 384)\n",
        "#pca_full = PCA(0, 384)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PqAJWMzyLI1w"
      },
      "source": [
        "plt.figure(figsize=(16, 4))\n",
        "plt.subplot(1,3,1)\n",
        "imgplot = plt.imshow(pca_1, cmap='Greys_r')\n",
        "plt.title(\"1 PCA\")\n",
        "\n",
        "plt.subplot(1,3,2)\n",
        "imgplot = plt.imshow(pca_1_20, cmap='Greys_r')\n",
        "plt.title(\"1-20 PCA\")\n",
        "\n",
        "plt.subplot(1,3,3)\n",
        "imgplot = plt.imshow(pca_1_50, cmap='Greys_r')\n",
        "plt.title(\"1-50 PCA\")\n",
        "\n",
        "plt.figure(figsize=(16, 4))\n",
        "plt.subplot(1,2,1)\n",
        "imgplot = plt.imshow(pca_20_100, cmap='Greys_r')\n",
        "plt.title(\"20-100 PCA\")\n",
        "\n",
        "plt.subplot(1,2,2)\n",
        "imgplot = plt.imshow(pca_20_end, cmap='Greys_r')\n",
        "plt.title(\"20-end PCA\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rqI_mXQhLI10"
      },
      "source": [
        "### What do you think 1st PCA component reflects? How do you find, is 1-50 components gives you a good image?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m8dBHwa5LI12"
      },
      "source": [
        "img.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XEvjvM1dLI16"
      },
      "source": [
        "First components reflects the biggest the place where one global objects transfers to another. At this place, the biggest gradient change happen.\n",
        "\n",
        "Using first components of the image, it is possible to compress it in size ie.\n",
        "\n",
        "using first K components will give memory gain\n",
        "\n",
        "$$\n",
        "\\frac{N_{rows} * N_{cols}}{K * (N_{rows} + N_{cols} + 1)}\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V6bqZBFDLI18"
      },
      "source": [
        "Interesting and helpful link"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9E2pIo7OLI19"
      },
      "source": [
        "[Medium](https://medium.com/@jonathan_hui/machine-learning-singular-value-decomposition-svd-principal-component-analysis-pca-1d45e885e491)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fql9PXMDLI1-"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}