{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "autoencoder_seminar.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yandexdataschool/MLatImperial2021/blob/master/08_lab/autoencoder_seminar.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xh9Ozzs3Cymq"
      },
      "source": [
        "import scipy as sp\n",
        "import scipy.misc\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TttuS1oqDMxy"
      },
      "source": [
        "import tensorflow as tf\n",
        "(X_train, y_train), (X_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
        "X_train = (X_train / 255).astype('float32')\n",
        "X_test  = (X_test  / 255).astype('float32')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3vD6h7mINmgU"
      },
      "source": [
        "device = torch.device(\"cuda:0\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jrXd2N0tNV8l"
      },
      "source": [
        "def preprocess_data(X, y, classification):\n",
        "  X_preprocessed = torch.tensor(X, dtype=torch.float).unsqueeze(1)\n",
        "  if classification:\n",
        "    y_preprocessed = torch.tensor(y, dtype=torch.long)\n",
        "  else:\n",
        "    y_preprocessed = torch.tensor(y).unsqueeze(1)\n",
        "  return X_preprocessed.to(device), y_preprocessed.to(device)\n",
        "\n",
        "def get_batches(X, y, batch_size, shuffle=False, classification=False):\n",
        "  if shuffle:\n",
        "    shuffle_ids = np.random.permutation(len(X))\n",
        "    X = X[shuffle_ids].copy()\n",
        "    y = y[shuffle_ids].copy()\n",
        "  for i_picture in range(0, len(X), batch_size):\n",
        "    # Get batch and preprocess it:\n",
        "    batch_X = X[i_picture:i_picture + batch_size]\n",
        "    batch_y = y[i_picture:i_picture + batch_size]\n",
        "    \n",
        "    # 'return' the batch (see the link above to\n",
        "    # better understand what 'yield' does)\n",
        "    yield preprocess_data(batch_X, batch_y, classification)  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b5bqzIRBL_Y_"
      },
      "source": [
        "from IPython.display import clear_output\n",
        "\n",
        "\n",
        "class Logger:\n",
        "  def __init__(self):\n",
        "    self.train_loss_batch = []\n",
        "    self.train_loss_epoch = []\n",
        "    self.test_loss_batch = []\n",
        "    self.test_loss_epoch = []\n",
        "    self.train_batches_per_epoch = 0\n",
        "    self.test_batches_per_epoch = 0\n",
        "    self.epoch_counter = 0\n",
        "\n",
        "  def fill_train(self, loss):\n",
        "    self.train_loss_batch.append(loss)\n",
        "    self.train_batches_per_epoch += 1\n",
        "\n",
        "  def fill_test(self, loss):\n",
        "    self.test_loss_batch.append(loss)\n",
        "    self.test_batches_per_epoch += 1\n",
        "\n",
        "  def finish_epoch(self):\n",
        "    self.train_loss_epoch.append(np.mean(\n",
        "        self.train_loss_batch[-self.train_batches_per_epoch:]\n",
        "    ))\n",
        "    self.test_loss_epoch.append(np.mean(\n",
        "        self.test_loss_batch[-self.test_batches_per_epoch:]\n",
        "    ))\n",
        "    self.train_batches_per_epoch = 0\n",
        "    self.test_batches_per_epoch = 0\n",
        "    \n",
        "    clear_output()\n",
        "  \n",
        "    print(\"epoch #{} \\t train_loss: {:.8} \\t test_loss: {:.8}\".format(\n",
        "              self.epoch_counter,\n",
        "              self.train_loss_epoch[-1],\n",
        "              self.test_loss_epoch [-1]\n",
        "          ))\n",
        "    \n",
        "    self.epoch_counter += 1\n",
        "\n",
        "    plt.figure(figsize=(11, 5))\n",
        "\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(self.train_loss_batch, label='train loss')\n",
        "    plt.xlabel('# batch iteration')\n",
        "    plt.ylabel('loss')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(self.train_loss_epoch, label='average train loss')\n",
        "    plt.plot(self.test_loss_epoch , label='average test loss' )\n",
        "    plt.legend()\n",
        "    plt.xlabel('# epoch')\n",
        "    plt.ylabel('loss')\n",
        "    plt.show();"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9PDNeI-TGmHQ"
      },
      "source": [
        "class Reshape(torch.nn.Module):\n",
        "  def __init__(self, *shape):\n",
        "    super(Reshape, self).__init__()\n",
        "    self.shape = shape\n",
        "\n",
        "  def forward(self, x):\n",
        "    return x.reshape(x.shape[0], *self.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UDsjA8TrDO-I"
      },
      "source": [
        "def create_encoder():\n",
        "    return torch.nn.Sequential(\n",
        "    nn.Conv2d(1, 16, 3, padding=1),\n",
        "    nn.LeakyReLU(),\n",
        "    nn.MaxPool2d(2), # 14x14\n",
        "\n",
        "    nn.Conv2d(16, 32, 3, padding=1),\n",
        "    nn.LeakyReLU(),\n",
        "    nn.MaxPool2d(2), # 7x7\n",
        "\n",
        "    nn.Conv2d(32, 64, 3), # 5x5\n",
        "    nn.LeakyReLU(),\n",
        "    nn.Conv2d(64, 128, 3), # 3x3\n",
        "    nn.LeakyReLU(),\n",
        "    nn.Conv2d(128,256, 3), # 1x1\n",
        "    nn.LeakyReLU(),\n",
        "    nn.Conv2d(256, 32, 1),\n",
        "\n",
        "    Reshape(32)\n",
        "  )\n",
        "\n",
        "def create_decoder():\n",
        "    return nn.Sequential(\n",
        "    Reshape(32, 1, 1),\n",
        "\n",
        "    nn.ConvTranspose2d(32, 256, 3, dilation=2), # 2x2\n",
        "    nn.LeakyReLU(),\n",
        "\n",
        "    nn.ConvTranspose2d(256, 128, 3, dilation=2), # 4x4\n",
        "    nn.LeakyReLU(),\n",
        "\n",
        "    nn.ConvTranspose2d(128, 64, 3, dilation=2), # 8x8\n",
        "    nn.LeakyReLU(),\n",
        "\n",
        "    nn.ConvTranspose2d(64, 32, 3, dilation=2), # 16x16\n",
        "    nn.LeakyReLU(),\n",
        "\n",
        "    nn.ConvTranspose2d(32, 16,3, dilation=2), # 28x28\n",
        "    nn.LeakyReLU(),\n",
        "    nn.ConvTranspose2d(16, 3,3, dilation=1), \n",
        "    nn.LeakyReLU(),\n",
        "    nn.ConvTranspose2d(3, 1,3, dilation=2), \n",
        "    nn.LeakyReLU(),\n",
        "    nn.ConvTranspose2d(1, 1,2, dilation=1),\n",
        "    nn.Sigmoid()\n",
        "  )\n",
        "\n",
        "\n",
        "encoder = create_encoder()\n",
        "decoder = create_decoder()\n",
        "\n",
        "autoencoder = torch.nn.Sequential(\n",
        "  encoder,\n",
        "  decoder\n",
        ").to(device)\n",
        "\n",
        "\n",
        "optimiser = torch.optim.Adam(autoencoder.parameters(), lr=0.003)\n",
        "loss_function = torch.nn.functional.mse_loss\n",
        "num_epochs = 20\n",
        "batch_size = 256"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oiGnnnkuMXjY"
      },
      "source": [
        "def fit(model, loss_function, optimizer, _X_train, _y_train, _X_test, _y_test, num_epochs, batch_size, classification=False):\n",
        "  logger = Logger()\n",
        "\n",
        "  for i_epoch in range(num_epochs):\n",
        "    model.train() # setting the model to training mode\n",
        "    for batch_X, batch_y in get_batches(_X_train, _y_train,\n",
        "                                        batch_size=batch_size, shuffle=True, classification=classification):\n",
        "      predictions = model(batch_X) # compute the predictions\n",
        "      loss = loss_function(predictions, batch_y) # compute the loss\n",
        "      logger.fill_train(loss.item())\n",
        "\n",
        "      model.zero_grad() # zero the gradients\n",
        "      loss.backward() # compute new gradients\n",
        "      optimizer.step() # do an optimization step\n",
        "\n",
        "    # Now, let's evaluate on the test part:\n",
        "    model.eval() # setting the model to evaluatioin mode\n",
        "    for batch_X, batch_y in get_batches(_X_test, _y_test,\n",
        "                                        batch_size=batch_size, classification=classification):\n",
        "      loss = loss_function(model(batch_X), batch_y)\n",
        "      logger.fill_test(loss.item())\n",
        "    \n",
        "    logger.finish_epoch()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tenugMcbMxdR"
      },
      "source": [
        "fit(autoencoder, loss_function, optimiser, X_train, X_train, X_test, X_test, num_epochs, batch_size, classification=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YA6v8mfNSaAM"
      },
      "source": [
        "X_test[:10].reshape(28, 280)\n",
        "\n",
        "plt.figure(figsize=(10, 10))\n",
        "plt.imshow(np.transpose(X_test[:10], (1,0,2)).reshape(28, 280), cmap='Greys')\n",
        "plt.axis('off');\n",
        "\n",
        "plt.figure(figsize=(10, 10))\n",
        "encoder_reconstruction = autoencoder(torch.tensor(X_test[:10]).unsqueeze(1).to(device)).cpu().detach()[:, 0, ...]\n",
        "\n",
        "plt.imshow(np.transpose(encoder_reconstruction, (1,0,2)).reshape(28, 280), cmap='Greys')\n",
        "plt.axis('off');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dag09IcbL19i"
      },
      "source": [
        "Now, lets make a classifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xAY0Yj9VVow3"
      },
      "source": [
        "for param in encoder.parameters():\n",
        "  param.requires_grad_(False)\n",
        "\n",
        "classifier = nn.Sequential(\n",
        "    encoder,\n",
        "    #nn.ReLU(),\n",
        "    nn.Linear(32, 10),\n",
        "    #nn.ReLU(),\n",
        "    #nn.Linear(10, 10)\n",
        "    ).to(device)\n",
        "\n",
        "optimiser = torch.optim.Adam(classifier.parameters(), lr=0.005)\n",
        "loss_function = torch.nn.functional.cross_entropy\n",
        "num_epochs = 70\n",
        "batch_size = 256\n",
        "\n",
        "\n",
        "fit(classifier, loss_function, optimiser, X_train[:300], y_train[:300], X_test, y_test, num_epochs, batch_size, classification=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pRl5dbtSawYS"
      },
      "source": [
        "## Test accuracy\n",
        "def get_accuracy(model, X, y):\n",
        "  return (torch.argmax(model(torch.tensor(X).unsqueeze(1).to(device)), dim=1).cpu().detach().numpy() == y).mean()\n",
        "\n",
        "print(get_accuracy(classifier, X_test, y_test))\n",
        "print(get_accuracy(classifier, X_train[:300], y_train[:300]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KgJ1y3NbY8Rt"
      },
      "source": [
        "encoder = create_encoder()\n",
        "\n",
        "for param in encoder.parameters():\n",
        "  param.requires_grad_(True)\n",
        "\n",
        "classifier = nn.Sequential(\n",
        "    encoder,\n",
        "    #nn.ReLU(),\n",
        "    nn.Linear(32, 10),\n",
        "    #nn.ReLU(),\n",
        "    #nn.Linear(10, 10)\n",
        "    ).to(device)\n",
        "\n",
        "optimiser = torch.optim.Adam(classifier.parameters(), lr=0.005)\n",
        "loss_function = torch.nn.functional.cross_entropy\n",
        "num_epochs = 70\n",
        "batch_size = 256\n",
        "\n",
        "\n",
        "fit(classifier, loss_function, optimiser, X_train[:300], y_train[:300], X_test, y_test, num_epochs, batch_size, classification=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "racs7DAPniTG"
      },
      "source": [
        "What do we observe on the training curve?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6nM-l8T3mfZL"
      },
      "source": [
        "print(get_accuracy(classifier, X_test, y_test))\n",
        "print(get_accuracy(classifier, X_train[:300], y_train[:300]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G9ZzQ5f7oRJw"
      },
      "source": [
        "Semi-supervised"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9FeZ2ds_axWT"
      },
      "source": [
        "X_train_labeled, X_train_unlabeled = X_train[:300], X_train[300:]\n",
        "y_train_labeled = y_train[:300]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z8h6_vLeazEu"
      },
      "source": [
        "def gen_untrained(batch_size):\n",
        "  ids = np.arange(len(X_train_unlabeled))\n",
        "  np.random.shuffle(ids)\n",
        "  for i in range(0, len(X_train_unlabeled), batch_size):\n",
        "    yield X_train_unlabeled[ids][i:i+batch_size]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iOSsxxKla0kp"
      },
      "source": [
        "unlabeled_generator = gen_untrained(256)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OWAgpqcKqGwe"
      },
      "source": [
        "Remember, what we want to do here is to create a class, that do two things: it acts both like a Autoencoder and classifier, so it should give you two outputs - a reconstructed image and classification probability vector"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Thomef25cJoK"
      },
      "source": [
        "class UnsupervisedAE(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.encoder = encoder\n",
        "    self.decoder = decoder\n",
        "    self.classifier = nn.Linear(32, 10)\n",
        "\n",
        "  def forward(self, X):\n",
        "    # <YOUR CODE>\n",
        "    return x_reco, x_class"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lIlYU5McqaWk"
      },
      "source": [
        "Define our losses"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "be4s3t5vcqAB"
      },
      "source": [
        "unsup_ae = UnsupervisedAE(create_encoder(), create_decoder()).to(device)\n",
        "\n",
        "\n",
        "optimiser = torch.optim.Adam(unsup_ae.parameters(), lr=0.003)\n",
        "\n",
        "mse_loss = torch.nn.functional.mse_loss\n",
        "ce_loss = torch.nn.functional.cross_entropy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iLXLQgpFbLZl"
      },
      "source": [
        "N_EPOCHS = 100\n",
        "BATCH_SIZE = 16\n",
        "\n",
        "LAMBDA = 0.3\n",
        "history_ae = []\n",
        "history_cl = []\n",
        "history_tot = []\n",
        "for i_epoch in range(N_EPOCHS):\n",
        "  print(\"Working on ep #\", i_epoch)\n",
        "  ids = np.arange(len(X_train_labeled))\n",
        "  np.random.shuffle(ids)\n",
        "\n",
        "  for i_image in range(0, len(X_train_labeled), BATCH_SIZE):\n",
        "    X_batch = torch.tensor(X_train_labeled[ids][i_image:i_image + BATCH_SIZE]).unsqueeze(1).to(device)\n",
        "    y_batch = torch.tensor(y_train_labeled[ids][i_image:i_image + BATCH_SIZE], dtype=torch.long).to(device)\n",
        "    try:\n",
        "      X_batch_unlabled = torch.tensor(unlabeled_generator.__next__()).unsqueeze(1).to(device)\n",
        "    except StopIteration:\n",
        "      unlabeled_generator = gen_untrained(256)\n",
        "      X_batch_unlabled = torch.tensor(unlabeled_generator.__next__()).unsqueeze(1).to(device)\n",
        "\n",
        "    epoch_ae_loss = 0\n",
        "    epoch_cl_loss = 0\n",
        "    epoch_total_loss = 0\n",
        "\n",
        "\n",
        "    # So, here we need to do two things: predict reconstructed image and our MSE loss on the UNLABELED dataset\n",
        "    reco_image, _ = unsup_ae(X_batch_unlabled)\n",
        "    ae_loss = mse_loss(reco_image, X_batch_unlabled)\n",
        "\n",
        "    # here, we want to predict the classification loss of the labeled data\n",
        "    _, class_preds = unsup_ae(X_batch)\n",
        "    cass_loss = ce_loss(class_preds, y_batch)\n",
        "\n",
        "    # And here we just want to make the sum of the losses with some regularisation coefficient\n",
        "    loss = cass_loss + LAMBDA * ae_loss\n",
        "\n",
        "    loss.backward()\n",
        "    optimiser.step()\n",
        "    unsup_ae.zero_grad()\n",
        "\n",
        "    epoch_ae_loss += ae_loss.item()\n",
        "    epoch_cl_loss += cass_loss.item()\n",
        "    epoch_total_loss += loss.item()\n",
        "  history_ae.append(epoch_ae_loss)\n",
        "  history_cl.append(epoch_cl_loss)\n",
        "  history_tot.append(epoch_total_loss)\n",
        "\n",
        "  if i_epoch % 1 == 0:\n",
        "    clear_output(wait=True)\n",
        "    plt.figure(figsize=(12, 8))\n",
        "    plt.plot(history_ae, label='ae loss')\n",
        "    plt.plot(history_cl, label='cl loss')\n",
        "    plt.plot(history_tot, label='total')\n",
        "    plt.legend()\n",
        "    plt.xlabel('epoch')\n",
        "    plt.ylabel('loss')\n",
        "    plt.show();"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F7I1eoX_d2T2"
      },
      "source": [
        "history_tot[-1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JtClk-Oph6wz"
      },
      "source": [
        "## Test accuracy\n",
        "def get_accuracy(model, X, y):\n",
        "  return (torch.argmax(model(torch.tensor(X).unsqueeze(1).to(device))[1], dim=1).cpu().detach().numpy() == y).mean()\n",
        "\n",
        "print(get_accuracy(unsup_ae, X_test, y_test))\n",
        "#print(get_accuracy(classifier, X_train[:300], y_train[:300]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1g16vJugiLNT"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}