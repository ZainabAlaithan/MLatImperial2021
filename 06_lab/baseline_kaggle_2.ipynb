{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "baseline_kaggle_2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yandexdataschool/MLatImperial2021/blob/master/06_lab/baseline_kaggle_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yRrjMtqAoXHx"
      },
      "source": [
        "## Do not forget to click Runtime -> Change Runtime type -> Hardware accelerator ->GPU"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JybtM5m4GXUO"
      },
      "source": [
        "# !Link to challenge!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dmaKRkuCGXUX"
      },
      "source": [
        "# https://www.kaggle.com/t/9d968fe3531b4a009ef0ccefca61f3ec"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gd_eJrz44j9h"
      },
      "source": [
        "As before - mount drive, download data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mUxPuR2IAQth"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vQ6NgIT96s1Y"
      },
      "source": [
        "DATA_PREFIX = \"/content/gdrive/My Drive/kaggle_2/\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3A6yDXrOaGgd"
      },
      "source": [
        "import os\n",
        "os.makedirs(DATA_PREFIX)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D0sdJcR-47d_"
      },
      "source": [
        "!mkdir /root/.kaggle\n",
        "!cp /content/gdrive/My\\ Drive/kaggle.json /root/.kaggle/\n",
        "!chmod 600 /root/.kaggle/kaggle.json\n",
        "!ls -l /root/.kaggle"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HZcU2zfcBV8l"
      },
      "source": [
        "!pip install --upgrade --force-reinstall --no-deps kaggle"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sv2fxrvm4-SR"
      },
      "source": [
        "!kaggle competitions download --force -c mlimperial2021-jet -p '{DATA_PREFIX}'\n",
        "\n",
        "!unzip -q '{DATA_PREFIX}/mlimperial2021-jet.zip' -d '{DATA_PREFIX}'\n",
        "\n",
        "!chmod +rw '{DATA_PREFIX}/kaggle_train.h5'\n",
        "!chmod +rw '{DATA_PREFIX}/kaggle_test.h5'\n",
        "!rm -r '{DATA_PREFIX}/mlimperial2021-jet.zip'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l_GMyDOnaSa7"
      },
      "source": [
        "!ls '{DATA_PREFIX}'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JvjMk58D5Ar4"
      },
      "source": [
        "# Create folder to save NN snapshots during training\n",
        "os.makedirs(os.path.join(\"/content/gdrive/My Drive/kaggle_2\", \"nn_snapshots\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hat06lTTrfnz"
      },
      "source": [
        "## Metric - ROC AUC"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZKjZkER2rdWb"
      },
      "source": [
        "Your task is to try as many techniques that you have learned this week as possible.\n",
        "\n",
        "The outcome of your work should be a small table with results, i.e \n",
        "Method - parameters tuned with CV - score + features created on top of exiting ones. The table should be accompanied by a small report of your workflow and reasoning. Also, you need to send the code.\n",
        "\n",
        "The archive with the files should be sent to mlicl-2021-seminars@yandex.ru with the topic: **Surname_name_kaggle_2**\n",
        "\n",
        "The total amount of points is 10. You will get additional points based on your final ranking.\n",
        "\n",
        "** 1 Point **\n",
        "\n",
        "Try different layers, such as Dropout, BN, different poolings, convs. Do all of them work? Why?\n",
        "\n",
        "** 1 Point **\n",
        "\n",
        "Try different activations, i.e. relu, sigmoid, tanh etc. Do all of them work? Why?\n",
        "\n",
        "** 1 Point **\n",
        "\n",
        "Try different optimisers, i.e. SGD, Adam, Adamax, rmsprop ... Which you find the best? Which converges faster?\n",
        "\n",
        "** 1 Point **\n",
        "\n",
        "Try different depth of network, different order of layers. What do you observe?\n",
        "\n",
        "** 3 Points **\n",
        "\n",
        "Augment(rotation, jitter, reflection ...) you data using symmetries in the data.\n",
        "\n",
        "** 1 Points **\n",
        "\n",
        "Change optimisation loop/optimiser to set up decay of learning rate by the scheme of your choice, abort trainin by stopping criteria of your choice.\n",
        "\n",
        "** 2 Points **\n",
        "\n",
        "Use pretrained nets / part of nest. Does this technique improve the score?\n",
        "\n",
        "** Bonus(if we make it:) ) **\n",
        "\n",
        "Beat medium baseline - + X bonus points.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G6SSIV_fxll-"
      },
      "source": [
        "# Util functions to preprocess and work with data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jcYBjvepxgEQ"
      },
      "source": [
        "def read_data(data, is_train=True, start_ind=0, end_ind=0):\n",
        "    layer_hcal = np.expand_dims(data['all_events']['histHCAL'][start_ind : end_ind], -1).astype(np.float32)\n",
        "    layer_em = np.expand_dims(data['all_events']['histEM'][start_ind : end_ind], -1).astype(np.float32)\n",
        "    layer_track = np.expand_dims(data['all_events']['histtrack'][start_ind : end_ind], -1).astype(np.float32)\n",
        "    \n",
        "    hit_map = np.concatenate((layer_hcal, layer_em, layer_track), axis=-1).astype(np.float32)\n",
        "    hit_map = np.rollaxis(hit_map, 3, 1)\n",
        "    hit_map = (hit_map - hit_map.mean(axis=0, keepdims=True)) / hit_map.std(axis=0, keepdims=True)\n",
        "    answers = None\n",
        "    if is_train:\n",
        "        answers = np.expand_dims(data['all_events']['y'][start_ind : end_ind], -1)\n",
        "    return hit_map, answers\n",
        "  \n",
        "def save_data_in_chunks(data, chunk_size):\n",
        "    for index, step in enumerate(range(0, len(data['all_events']['histHCAL']), chunk_size)):\n",
        "        X, y = read_data(train, True, step, step + chunk_size)\n",
        "        X_train, X_val, y_train, y_val = train_test_split(X, y, train_size=0.9, random_state=42)\n",
        "        np.save(\"X_train_{}\".format(index) , X_train)\n",
        "        np.save(\"y_train_{}\".format(index) , y_train)        \n",
        "        np.save(\"X_val_{}\".format(index) , X_val)\n",
        "        np.save(\"y_val_{}\".format(index) , y_val)\n",
        "        del X, y, X_train, X_val, y_train, y_val\n",
        "        gc.collect()\n",
        "        print(\"Done:{}\".format(index))  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w248AVRhGXUd"
      },
      "source": [
        "### import standard packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LwmX7UoxGXVO"
      },
      "source": [
        "import gc\n",
        "import numpy as np\n",
        "import pickle\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import scipy\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3jo5_KXSGXVV"
      },
      "source": [
        "## import torch and h5 data format parser"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YfQUq4w9GXVX"
      },
      "source": [
        "import h5py\n",
        "import torch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NvVq5p2SyGFM"
      },
      "source": [
        "### Create small batches of train data locally(will be deleted after restrat of the session)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EQjimmEQyBWd"
      },
      "source": [
        "train = h5py.File(os.path.join(DATA_PREFIX, \"kaggle_train.h5\"), 'r')\n",
        "save_data_in_chunks(train, 50000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L0BfrMr4eF8j"
      },
      "source": [
        "print(len(train['all_events']['histHCAL']))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vBM73vB_Rws_"
      },
      "source": [
        "Event example"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GxTvYZLOGXVl"
      },
      "source": [
        "X_example = np.load(\"X_train_0.npy\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gNOFRAnhGXVq"
      },
      "source": [
        "f, ax = plt.subplots(1,3,figsize=(14,6))\n",
        "for i in range(3):\n",
        "    ax[i].imshow(X_example[100,i,:,:], cmap=\"Reds\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vPlYByCvh8XP"
      },
      "source": [
        "del X_example\n",
        "gc.collect()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1a99yYfk5m-w"
      },
      "source": [
        "Load validation data to memory"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dp-SnS_-5lzZ"
      },
      "source": [
        "# This variable tells, how many file pieces of validation data we want to consider\n",
        "N_DATA_SPLITS = 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xc3rKXrlGXVx"
      },
      "source": [
        "X_val = np.concatenate([np.load(\"X_val_{}.npy\".format(i)) for i in range(N_DATA_SPLITS)])\n",
        "y_val = np.concatenate([np.load(\"y_val_{}.npy\".format(i)) for i in range(N_DATA_SPLITS)])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lwKUQnMDGXV0"
      },
      "source": [
        "## The next steps are exactly the same as on  Monday"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zSD1Anc8GXV1"
      },
      "source": [
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class Flatten(nn.Module):\n",
        "    def forward(self, input):\n",
        "        return input.view(input.size(0), -1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZZQZ7WmqGXV7"
      },
      "source": [
        "### This is dummy example of CNN, look what you can change here"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zf_m7XRAGXV8"
      },
      "source": [
        "device = torch.device(\"cuda\", 0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U1KXoC7pGXWA"
      },
      "source": [
        "model = torch.nn.Sequential()\n",
        "model.add_module(\"maxpool_1\", torch.nn.MaxPool2d(kernel_size=2))\n",
        "model.add_module('conv_1', nn.Conv2d(3, 32, kernel_size=(5,5), stride=1, padding=0))\n",
        "model.add_module(\"maxpool_2\", torch.nn.MaxPool2d(kernel_size=2))\n",
        "model.add_module(\"relu_1\", torch.nn.ReLU())\n",
        "\n",
        "model.add_module(\"flat\", Flatten())\n",
        "\n",
        "model.add_module(\"fc1\", torch.nn.Linear(6272, 128))\n",
        "model.add_module(\"relu_2\", torch.nn.ReLU())\n",
        "model.add_module(\"fc2\", torch.nn.Linear(128, 1))\n",
        "model.add_module(\"sigmoid\", torch.nn.Sigmoid())\n",
        "\n",
        "model.to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PvdHX2c_GXWI"
      },
      "source": [
        "### Training on minibatches\n",
        "\n",
        "Just like before, we train our model on small random minibatches of data with adaptive optimization method of your choice."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qFUuixd6RU31"
      },
      "source": [
        "# An auxilary function that returns mini-batches for neural network training\n",
        "from tqdm import trange\n",
        "def iterate_minibatches(X, y, batchsize, shuffle=False):\n",
        "    indices = np.arange(len(X))\n",
        "    if shuffle: \n",
        "        indices = np.random.permutation(indices)\n",
        "    for start in trange(0, len(indices), batchsize):\n",
        "        ix = indices[start: start + batchsize]\n",
        "        yield X[ix], y[ix]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E6z_3dT3GXWJ"
      },
      "source": [
        "### Choose you optimiser"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zOr4UI8uGXWL"
      },
      "source": [
        "opt = torch.optim.SGD(model.parameters(), lr=0.01)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iCRa5DFAGXWQ"
      },
      "source": [
        "## And set up batch_size and number of epochs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rB_XDDUXGXWS"
      },
      "source": [
        "import time\n",
        "#from pandas import ewma\n",
        "from IPython import display\n",
        "\n",
        "num_epochs = 2 #amount of passes through the data\n",
        "batch_size = 1024 #number of samples processed at each function call\n",
        "auc_history = []\n",
        "\n",
        "number_of_chunks = 1 #number of initial data splits to process\n",
        "best_score = 0\n",
        "best_epoch = 0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ztkDPzhqGXWY"
      },
      "source": [
        "for epoch in range(num_epochs):\n",
        "    # In each epoch, we do a full pass over the training data:\n",
        "    \n",
        "    train_err = train_acc = 0\n",
        "    train_batches = 0\n",
        "    start_time = time.time()\n",
        "    \n",
        "    for step in range(number_of_chunks):\n",
        "        X_train, y_train = np.load(\"X_train_{}.npy\".format(step)), np.load(\"y_train_{}.npy\".format(step))\n",
        "        train_batches += np.ceil(len(X_train) / batch_size).astype(int)\n",
        "        # This is you have see already - traning loop\n",
        "        model.train(True) # enable dropout / batch_norm training behavior\n",
        "        for X_batch, y_batch in iterate_minibatches(X_train, y_train, batch_size, shuffle=True):\n",
        "            X_batch = torch.FloatTensor(X_batch).to(device)\n",
        "            y_batch = torch.FloatTensor(y_batch).to(device)\n",
        "\n",
        "            y_predicted = model(X_batch)\n",
        "            loss = torch.nn.functional.binary_cross_entropy(y_predicted, y_batch).mean()\n",
        "\n",
        "            loss.backward()\n",
        "            opt.step()\n",
        "            opt.zero_grad()\n",
        "\n",
        "            train_err += loss.data.cpu().numpy()\n",
        "            train_acc += torch.eq(torch.round(y_predicted), y_batch).data.cpu().numpy().mean()\n",
        "\n",
        "    # And a full pass over the validation data:\n",
        "    y_pred = []\n",
        "\n",
        "    model.train(False)\n",
        "    for X_batch, y_batch in iterate_minibatches(X_val, y_val, batch_size, shuffle=False):\n",
        "        X_batch = torch.FloatTensor(X_batch).to(device)\n",
        "        y_pred.extend(model(X_batch).data.cpu().numpy())\n",
        "\n",
        "    y_pred = np.asarray(y_pred)\n",
        "    # Save the metrics values   \n",
        "    val_acc = accuracy_score(y_val, y_pred > 0.5)\n",
        "    val_roc_auc = roc_auc_score(y_val, y_pred)\n",
        "    auc_history.append(val_roc_auc)\n",
        "\n",
        "\n",
        "    # Visualize\n",
        "    display.clear_output(wait=True)\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    plt.title(\"Validation AUC\")\n",
        "    plt.xlabel(\"#iteration\")\n",
        "    plt.ylabel(\"AUC\")\n",
        "    plt.plot(auc_history, 'b',label='val auc')\n",
        "    plt.legend(loc='best')\n",
        "    plt.grid()\n",
        "    plt.show()\n",
        "\n",
        "    # Then we print the results for this epoch:\n",
        "    print(\"Epoch {} of {} took {:.3f}s\".format(\n",
        "        epoch + 1, num_epochs, time.time() - start_time))\n",
        "\n",
        "    print(\"  training loss (in-iteration):\\t\\t{:.6f}\".format(train_err / train_batches))\n",
        "    print(\"  train accuracy:\\t\\t{:.2f} %\".format(train_acc / train_batches * 100))\n",
        "    print(\"  validation accuracy:\\t\\t{:.2f} %\".format(val_acc * 100))\n",
        "    print(\"  validation roc_auc:\\t\\t{:.2f} %\".format(val_roc_auc * 100))\n",
        "    \n",
        "    \n",
        "    if auc_history[-1] > best_score:\n",
        "        best_score = auc_history[-1]\n",
        "        best_epoch = epoch\n",
        "        with open(os.path.join(\"/content/gdrive/My Drive/kaggle_2\", \"nn_snapshots\", \"best.pt\"), 'wb') as f:\n",
        "            torch.save(model, f)    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rCNaPjLOGXWi"
      },
      "source": [
        "# Read test data, feed it to neural network, and save the output in kaggle fromat."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TFRCSIl_yjOW"
      },
      "source": [
        "chunk_size = 10000\n",
        "test = h5py.File(os.path.join(DATA_PREFIX, \"kaggle_test.h5\"), 'r')\n",
        "\n",
        "y_ans = []\n",
        "for index, step in enumerate(range(0, len(test['all_events']['histHCAL']), chunk_size)):\n",
        "    X, _ = read_data(test, False, step, step + chunk_size)\n",
        "    y_ans.extend(model(torch.FloatTensor(X).to(device).contiguous()).detach().cpu().numpy())\n",
        "    del X\n",
        "    gc.collect()\n",
        "    print(\"Done:{}\".format(index))\n",
        "    \n",
        "y_ans = np.array(y_ans)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GyvyG1VJGXWq"
      },
      "source": [
        "Saving you results to file."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CoGjXmOYGXWr"
      },
      "source": [
        "import pandas as pd\n",
        "from IPython.display import FileLink\n",
        "\n",
        "def save_results(filename, y_ans):\n",
        "    answer_dataframe = pd.DataFrame(columns=[\"ID\", \"ans\"])\n",
        "    answer_dataframe['ID'] = range(0,len(y_ans))\n",
        "    answer_dataframe['ans'] = y_ans\n",
        "    answer_dataframe.to_csv(os.path.join(DATA_PREFIX, '{}'.format(filename)), index=False)\n",
        "    return FileLink(os.path.join(DATA_PREFIX, '{}'.format(filename)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G00EVpafGXWv"
      },
      "source": [
        "save_results(\"baseline.csv\", y_ans)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-INVt7xj6RRy"
      },
      "source": [
        "!kaggle competitions submit -c mlimperial2021-jet -f \"{DATA_PREFIX}/baseline.csv\" -m \"Message\""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}