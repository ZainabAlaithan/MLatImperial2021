{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "lab6_Conv.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yandexdataschool/MLatImperial2021/blob/master/06_lab/lab6_Conv.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yFe9irgz6Iyi"
      },
      "source": [
        "### Recap"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hntoJ2Ix4PfR"
      },
      "source": [
        "Today we'll play around with convolutional neural networks, but before we do that let's start from where we left off yesterday.\n",
        "\n",
        "The cells below contain some code that we came up with in the yesterday's notebook."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pkASxl6K5CaA"
      },
      "source": [
        "Some imports:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4jRTV00-OsGB"
      },
      "source": [
        "from torchvision.datasets import FashionMNIST\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import torch\n",
        "print(torch.__version__)\n",
        "from IPython.display import clear_output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0XvCareYbeOa"
      },
      "source": [
        "if torch.cuda.is_available():\n",
        "  device = torch.device('cuda')\n",
        "else:\n",
        "  print(\"WARNING: gpu not found, the code will run on cpu\")\n",
        "  device = torch.device('cpu')\n",
        "\n",
        "print(f'Device is: \"{device}\".')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e3WH_yAi5N4F"
      },
      "source": [
        "A utility class to monitor the training procedure:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gQ4KpNKs45ir"
      },
      "source": [
        "from IPython.display import clear_output\n",
        "\n",
        "class Logger:\n",
        "  def __init__(self):\n",
        "    self.train_loss_batch = []\n",
        "    self.train_loss_epoch = []\n",
        "    self.test_loss_batch = []\n",
        "    self.test_loss_epoch = []\n",
        "    self.train_batches_per_epoch = 0\n",
        "    self.test_batches_per_epoch = 0\n",
        "    self.epoch_counter = 0\n",
        "\n",
        "  def fill_train(self, loss):\n",
        "    self.train_loss_batch.append(loss)\n",
        "    self.train_batches_per_epoch += 1\n",
        "\n",
        "  def fill_test(self, loss):\n",
        "    self.test_loss_batch.append(loss)\n",
        "    self.test_batches_per_epoch += 1\n",
        "\n",
        "  def finish_epoch(self, make_plot=True):\n",
        "    self.train_loss_epoch.append(np.mean(\n",
        "        self.train_loss_batch[-self.train_batches_per_epoch:]\n",
        "    ))\n",
        "    self.test_loss_epoch.append(np.mean(\n",
        "        self.test_loss_batch[-self.test_batches_per_epoch:]\n",
        "    ))\n",
        "    self.train_batches_per_epoch = 0\n",
        "    self.test_batches_per_epoch = 0\n",
        "    \n",
        "    if make_plot:\n",
        "      clear_output()\n",
        "  \n",
        "    print(\"epoch #{} \\t train_loss: {:.8} \\t test_loss: {:.8}\".format(\n",
        "              self.epoch_counter,\n",
        "              self.train_loss_epoch[-1],\n",
        "              self.test_loss_epoch [-1]\n",
        "          ))\n",
        "    \n",
        "    self.epoch_counter += 1\n",
        "\n",
        "    if make_plot:\n",
        "      plt.figure(figsize=(11, 5))\n",
        "\n",
        "      plt.subplot(1, 2, 1)\n",
        "      plt.plot(self.train_loss_batch, label='train loss')\n",
        "      plt.xlabel('# batch iteration')\n",
        "      plt.ylabel('loss')\n",
        "      plt.legend()\n",
        "\n",
        "      plt.subplot(1, 2, 2)\n",
        "      plt.plot(self.train_loss_epoch, label='average train loss')\n",
        "      plt.plot(self.test_loss_epoch , label='average test loss' )\n",
        "      plt.legend()\n",
        "      plt.xlabel('# epoch')\n",
        "      plt.ylabel('loss')\n",
        "      plt.show();"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0o_l1fZj5S_8"
      },
      "source": [
        "Data preprocessing:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AQMRQtc948Xr"
      },
      "source": [
        "def preprocess_data(X, y):\n",
        "  X_preprocessed = torch.tensor(X / 255.,\n",
        "                                dtype=torch.float).reshape(-1, 28 * 28)\n",
        "  y_preprocessed = torch.tensor(y)\n",
        "  return X_preprocessed.to(device), y_preprocessed.to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qsg8p7EH5c0G"
      },
      "source": [
        "Batch generator:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bOG5I0_G40fM"
      },
      "source": [
        "# Batch generator\n",
        "# (here's a very brief description of what python generators are:\n",
        "# https://stackoverflow.com/a/231855/3801744)\n",
        "def get_batches(X, y, batch_size, shuffle=False):\n",
        "  if shuffle:\n",
        "    shuffle_ids = np.random.permutation(len(X))\n",
        "    X = X[shuffle_ids].copy()\n",
        "    y = y[shuffle_ids].copy()\n",
        "  for i_picture in range(0, len(X), batch_size):\n",
        "    # Get batch and preprocess it:\n",
        "    batch_X = X[i_picture:i_picture + batch_size]\n",
        "    batch_y = y[i_picture:i_picture + batch_size]\n",
        "    \n",
        "    # 'return' the batch (see the link above to\n",
        "    # better understand what 'yield' does)\n",
        "    yield preprocess_data(batch_X, batch_y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j1ikMsY05nX_"
      },
      "source": [
        "Getting the data:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nVqelB915AVr"
      },
      "source": [
        "# Getting the train and test parts of the dataset\n",
        "data_train = FashionMNIST(\"FashionMNIST/\",\n",
        "                          download=True,\n",
        "                          train=True)\n",
        "\n",
        "data_test = FashionMNIST(\"FashionMNIST/\",\n",
        "                          download=True,\n",
        "                          train=False)\n",
        "\n",
        "# In fact, it's already stored as torch tensor, but we'll need\n",
        "# to work with the numpy representation, so let's do the convertion:\n",
        "X_train = data_train.train_data.numpy()\n",
        "y_train = data_train.train_labels.numpy()\n",
        "\n",
        "X_test = data_test.test_data.numpy()\n",
        "y_test = data_test.test_labels.numpy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IGmUq5b05sAL"
      },
      "source": [
        "Getting predictions on the test data:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nzKtgKEr5ZPE"
      },
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "def get_test_predictions(model, batch_size=100):\n",
        "  predictions_test = np.concatenate([\n",
        "    model(batch_X).to('cpu').detach().numpy()\n",
        "    for batch_X, batch_y in get_batches(X_test, y_test, batch_size)\n",
        "  ], axis=0)\n",
        "  return np.argmax(predictions_test, axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iDlPEsP15vqR"
      },
      "source": [
        "And finally, the training procedure (we've put it into a function to re-use with different models):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TR-h1LaUOaU2"
      },
      "source": [
        "def fit(model, loss_function, optimizer, num_epochs, batch_size, plot_every=5):\n",
        "  logger = Logger()\n",
        "\n",
        "  for i_epoch in range(num_epochs):\n",
        "    model.train() # setting the model to training mode\n",
        "    for batch_X, batch_y in get_batches(X_train, y_train,\n",
        "                                        batch_size=batch_size, shuffle=True):\n",
        "      predictions = model(batch_X) # compute the predictions\n",
        "      loss = loss_function(predictions, batch_y) # compute the loss\n",
        "      logger.fill_train(loss.item())\n",
        "\n",
        "      model.zero_grad() # zero the gradients\n",
        "      loss.backward() # compute new gradients\n",
        "      optimizer.step() # do an optimization step\n",
        "\n",
        "    # Now, let's evaluate on the test part:\n",
        "    model.eval() # setting the model to evaluatioin mode\n",
        "    for batch_X, batch_y in get_batches(X_test, y_test,\n",
        "                                        batch_size=batch_size):\n",
        "      loss = loss_function(model(batch_X), batch_y)\n",
        "      logger.fill_test(loss.item())\n",
        "    \n",
        "    logger.finish_epoch(make_plot=(i_epoch % plot_every == 0 or i_epoch == num_epochs - 1))\n",
        "  \n",
        "  # Counting model parameters\n",
        "  n_parameters = 0\n",
        "  for parameter in model.parameters():\n",
        "    n_parameters += parameter.reshape(-1).shape[0]\n",
        "\n",
        "  print(\"# of trainable parameters:\", n_parameters)\n",
        "\n",
        "  model.eval()\n",
        "  print(\"Model accuracy:\", accuracy_score(y_test, get_test_predictions(model)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2qVKqwxh6VyW"
      },
      "source": [
        "#### Yesterday's model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JrPLTaoE6a36"
      },
      "source": [
        "# Defining the loss function:\n",
        "loss_function = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "# Defining the model\n",
        "input_size = 28 * 28 # number of pixels per image\n",
        "output_size = 10 # number of classes\n",
        "n_hidden = 100\n",
        "\n",
        "model = torch.nn.Sequential(\n",
        "    torch.nn.Linear(in_features=input_size,\n",
        "                    out_features=n_hidden),\n",
        "    torch.nn.ELU(),\n",
        "    torch.nn.Dropout(p=0.2),\n",
        "    torch.nn.Linear(in_features=n_hidden,\n",
        "                    out_features=output_size),\n",
        ").to(device)\n",
        "\n",
        "# Setting up the optimizer\n",
        "learning_rate = 0.005\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "fit(model, loss_function, optimizer, num_epochs=20, batch_size=100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bEtx0XvANxbY"
      },
      "source": [
        "### Convolutional layers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LkE_JQKUNzB-"
      },
      "source": [
        "<img src='https://cdn-images-1.medium.com/max/1600/0*iqNdZWyNeCr5tCkc.' alt='CNN animation'>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MPJvoTAtaW5u"
      },
      "source": [
        "One filter is applied to all the channels of the input image image, i.e.:\n",
        "$$\n",
        "V(x,y,t) = \\sum_{i=x - \\delta}^{x + \\delta} \\sum_{j=y - \\delta}^{y + \\delta} \\sum_{s=1}^S K^t(i - x + \\delta, j - y + \\delta, s) \\cdot U(i,j,s)\n",
        "$$\n",
        "\n",
        "\n",
        "the output value V in position x, y and channel t will be calculated by the formula above. U is the input image, K is the kernel "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y43sj00oXgcZ"
      },
      "source": [
        "<font color='red'>Question:</font>What will be the output size of the:\n",
        "\n",
        "- picture of size 1x3x3, applied conv filter 3x3, stride=1, padding=0\n",
        "- picture of size 1x10x10, applied conv filter 3x3, stride=1, padding=0\n",
        "- picture of size 1x10x10, applied conv filter 3x3, stride=1, padding=1\n",
        "\n",
        "- picture of size 3x20x20, applied conv filter 3x3, stride=3, padding=0\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cECqaRmrZTua"
      },
      "source": [
        "Eventually, this is the formula to calculate output size\n",
        "$$\n",
        "Output_{size} = \\frac{(W−K+2P)}{S}+1\n",
        "$$\n",
        "\n",
        "W is the input size (H or W)\n",
        "K is the Kernel size\n",
        "P is the padding\n",
        "S is the stride"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "24tBqv1IN33J"
      },
      "source": [
        "<img src='https://cdn-images-1.medium.com/max/2000/1*vkQ0hXDaQv57sALXAJquxA.jpeg' alt='img'>\n",
        "(image taken from https://towardsdatascience.com/a-comprehensive-guide-to-convolutional-neural-networks-the-eli5-way-3bd2b1164a53)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kTKgl8r7EYrq"
      },
      "source": [
        "A general view of the most common convolutional architecture is shown above. The main idea is to gradually reduce the size of the image while increasing the number of channels. This is motivated by the following:\n",
        "\n",
        " - It's expensive (in terms of memory) to make a lot of channels for a large image, while smaller sized images allow us to do so. Intuitively, there's a trade-off between image size and number of channels.\n",
        " - We actually don't need that many channels at lower levels since there's not that many distinct low-level features for an image. Higher level features are more complex and require more filters (channels).\n",
        " - At the left side of the diagram (for low-level features) we care more about the positional information (e.g. \"is this stroke located near that one?\"), while at the right side (high-level features) we want to know what kind of an object we see, rather than where exactly we see it (e.g. \"looks like there's furry face somewhere in this picture - I might be looking at a cat\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dD5Djx8CIleO"
      },
      "source": [
        "#### Getting a grip on convolutions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yYsdbRX6NFAJ"
      },
      "source": [
        "Let's get an image of a cat:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e1stMSxMM7y8"
      },
      "source": [
        "!wget https://github.com/yandexdataschool/MLatImperial2021/raw/master/06_lab/cat.jpg"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NxwTWHuLKB5E"
      },
      "source": [
        "from matplotlib.pyplot import imread\n",
        "from skimage import color\n",
        "\n",
        "\n",
        "img = color.rgb2grey(imread('cat.jpg')).astype(np.float32)\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "plt.imshow(img, cmap='Greys_r');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3xiQOCWeNIMC"
      },
      "source": [
        "Now convolve this image with a single 3x3 filter and check out the result:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Em5Djcx-NU-Q"
      },
      "source": [
        "kernel = torch.Tensor([[1., 0., -1.],\n",
        "                       [1., 0., -1.],\n",
        "                       [1., 0., -1.]]).unsqueeze(0).unsqueeze(0)\n",
        "\n",
        "convolved_image = <YOUR CODE> # Use torch.nn.functional.conv2d.\n",
        "                              # Don't forget to convert torch tensor\n",
        "                              # to a numpy array afterwards.\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "plt.imshow(convolved_image);\n",
        "plt.figure(figsize=(12, 8))\n",
        "plt.imshow(convolved_image > 0.2);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zl4vI5liPMVj"
      },
      "source": [
        "Once you're done try other kernels and see how they affect the image – what features do they highlight?\n",
        "\n",
        "Some suggestions:\n",
        "```\n",
        "[[ 0., -1.,  0.],\n",
        " [-1.,  4., -1.],\n",
        " [ 0., -1.,  0.]]\n",
        "```\n",
        "\n",
        "```\n",
        "[[-2., -1.,  0.],\n",
        " [-1.,  0.,  1.],\n",
        " [ 0.,  1.,  2.]]\n",
        "```\n",
        "\n",
        "What will happen if you apply a convolution twice? `n` times?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kXGyy279ROrL"
      },
      "source": [
        "#### Building a CNN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "elM80oLHN6xb"
      },
      "source": [
        "Convolutional layers in torch expect their input to be of 4-dimensional shape: $(B, C, H, W)$. Here $B$ is the number images per batch, $C$ is the number of channels (e.g. 1 for a greyscale image, 3 for an RGB one, or number of filters from the previous convolutional layer). $H$ and $W$ are height and width in pixels.\n",
        "\n",
        "This means, at the beggining of our network we need to reshape our images from $(B, 784)$ to $(B, 1, 28, 28)$. In the end we'll want to reshape it back to 2 dimensions in order to apply a linear connection.\n",
        "\n",
        "For some reason torch doesn't have a reshaping layer, so we'll implement our own:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9dIr4xVPN_LZ"
      },
      "source": [
        "class Reshape(torch.nn.Module):\n",
        "  def __init__(self, *shape):\n",
        "    super(Reshape, self).__init__()\n",
        "    self.shape = shape\n",
        "\n",
        "  def forward(self, x):\n",
        "    return x.reshape(x.shape[0], *self.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xMgkQ_sI-z0I"
      },
      "source": [
        "Ok, now let's create and train a convolutional NN!\n",
        "\n",
        "Do keep in mind the model architecture from the picture above. I.e. we want to gradually reduce the size of the image while increasing the number of channels. We also want at least one fully connected layer at the end of the network.\n",
        "\n",
        "Use `torch.nn.Conv2d` for convolutions and `torch.nn.MaxPool2d` for max pooling."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZTdPa76a-9qP"
      },
      "source": [
        "# Define the hyperparameters of our model:\n",
        "learning_rate = 0.005\n",
        "dropout_rate = 0.2\n",
        "\n",
        "# Define the model itself\n",
        "model = torch.nn.Sequential(\n",
        "            Reshape(1, 28, 28),\n",
        "\n",
        "            torch.nn.Conv2d(<YOUR CODE>),\n",
        "            torch.nn.ELU(),\n",
        "            torch.nn.Dropout(p=dropout_rate),\n",
        "            torch.nn.MaxPool2d(2),\n",
        "\n",
        "            <YOUR CODE> # add more convolutions and max-poolings\n",
        "\n",
        "            Reshape(512),\n",
        "            torch.nn.Linear(512, 10)\n",
        "        ).to(device)\n",
        "\n",
        "# Creating the instances of the loss function and optimizer\n",
        "loss_function = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "fit(model, loss_function, optimizer, num_epochs=20, batch_size=100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_7aVGl5gOPQL"
      },
      "source": [
        "predictions_test = get_test_predictions(model)\n",
        "\n",
        "wrongly_predicted = X_test[predictions_test != y_test]\n",
        "wrongly_predicted_labels = predictions_test[predictions_test != y_test]\n",
        "wrongly_predicted_labels_true = y_test[predictions_test != y_test]\n",
        "\n",
        "label_names = np.array([\n",
        "    'T-shirt/top',\n",
        "    'Trouser',\n",
        "    'Pullover',\n",
        "    'Dress',\n",
        "    'Coat',\n",
        "    'Sandal',\n",
        "    'Shirt',\n",
        "    'Sneaker',\n",
        "    'Bag',\n",
        "    'Ankle boot',\n",
        "])\n",
        "\n",
        "\n",
        "# Print and plot the first 100:\n",
        "wrongly_predicted_labels = \\\n",
        "  label_names[wrongly_predicted_labels[:100]].reshape(10, 10)\n",
        "wrongly_predicted_labels_true = \\\n",
        "  label_names[wrongly_predicted_labels_true[:100]].reshape(10, 10)\n",
        "\n",
        "for ix in range(10):\n",
        "  for iy in range(10):\n",
        "    plt.text(ix / 5, iy / 8, (wrongly_predicted_labels[-1 - iy, ix]))\n",
        "    plt.text(ix / 5, iy / 8 + 0.05, (wrongly_predicted_labels_true[-1 - iy, ix]), color='red')\n",
        "plt.axis('off')\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(12,12))\n",
        "plt.imshow(wrongly_predicted[:100].reshape(10, 10, 28, 28)\n",
        "           .transpose(0, 2, 1, 3).reshape(280, 280), cmap=\"Greys\")\n",
        "plt.axis('off');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NHtfilanoX2I"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}