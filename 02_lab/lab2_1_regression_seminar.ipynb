{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.2"
    },
    "colab": {
      "name": "lab2_1_regression_seminar.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yandexdataschool/MLatImperial2021/blob/master/02_lab/lab2_1_regression_seminar.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BRCKrupOesLn"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cz1LUfwtesLs"
      },
      "source": [
        "# Today's data\n",
        "\n",
        "400 fotos of human faces. Each face is a 2d array [64x64] of pixel brightness."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "melNrhQoesLt"
      },
      "source": [
        "from sklearn.datasets import fetch_olivetti_faces\n",
        "data = fetch_olivetti_faces().images"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XA2Q-6P2esLy"
      },
      "source": [
        "# @this code showcases matplotlib subplots. The syntax is: plt.subplot(height, width, index_starting_from_1)\n",
        "plt.subplot(2,2,1)\n",
        "plt.imshow(data[0],cmap='gray')\n",
        "plt.subplot(2,2,2)\n",
        "plt.imshow(data[1],cmap='gray')\n",
        "plt.subplot(2,2,3)\n",
        "plt.imshow(data[2],cmap='gray')\n",
        "plt.subplot(2,2,4)\n",
        "plt.imshow(data[3],cmap='gray')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6hbCClhYesL5"
      },
      "source": [
        "# Face reconstruction problem\n",
        "\n",
        "Let's solve the face reconstruction problem: given left halves of facex __(X)__, our algorithm shall predict the right half __(y)__. Our first step is to slice the photos into X and y using slices.\n",
        "\n",
        "__Slices in numpy:__\n",
        "* In regular python, slice looks roughly like this: `a[2:5]` _(select elements from 2 to 5)_\n",
        "* Numpy allows you to slice N-dimensional arrays along each dimension: [image_index, height, width]\n",
        "  * `data[:10]` - Select first 10 images\n",
        "  * `data[:, :10]` - For all images, select a horizontal stripe 10 pixels high at the top of the image\n",
        "  * `data[10:20, :, -25:-15]` - Take images [10, 11, ..., 19], for each image select a _vetrical stripe_ of width 10 pixels, 15 pixels away from the _right_ side.\n",
        "\n",
        "__Your task:__\n",
        "\n",
        "Let's use slices to select all __left image halves as X__ and all __right halves as y__."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eV3HsLhQesL6"
      },
      "source": [
        "data.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QUUu5T-uesL-"
      },
      "source": [
        "# select left half of each face as X, right half as Y\n",
        "X = <Slice left half-images>\n",
        "y = <Slice right half-images>"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cW4F11b_esMD"
      },
      "source": [
        "# If you did everything right, you're gonna see left half-image and right half-image drawn separately in natural order\n",
        "plt.subplot(1,2,1)\n",
        "plt.imshow(X[0],cmap='gray')\n",
        "plt.subplot(1,2,2)\n",
        "plt.imshow(y[0],cmap='gray')\n",
        "\n",
        "assert X.shape == y.shape == (len(data), 64, 32), \"Please slice exactly the left half-face to X and right half-face to Y\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W2Rxit8SesMH"
      },
      "source": [
        "def glue(left_half,right_half):\n",
        "    # merge photos back together\n",
        "    left_half = left_half.reshape([-1, 64, 32])\n",
        "    right_half = right_half.reshape([-1, 64, 32])\n",
        "    return np.concatenate([left_half, right_half], axis=-1)\n",
        "\n",
        "\n",
        "# if you did everything right, you're gonna see a valid face\n",
        "plt.imshow(glue(X, y)[99], cmap='gray')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CcvCMAogesML"
      },
      "source": [
        "# Machine learning stuff"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N_ajG2zoesMM"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X.reshape([len(X), -1]),\n",
        "                                                    y.reshape([len(y), -1]),\n",
        "                                                    test_size=0.05, random_state=42)\n",
        "\n",
        "print(X_test.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3timNAgJesMR"
      },
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "model = LinearRegression()\n",
        "model.fit(X_train, Y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JPXoDAiqesMW"
      },
      "source": [
        "measure mean squared error"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PH2HuGi4esMY"
      },
      "source": [
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "print(\"Train MSE:\", mean_squared_error(Y_train, model.predict(X_train)))\n",
        "print(\"Test MSE:\", mean_squared_error(Y_test, model.predict(X_test)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cCvibaLEfyIV"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K-e00MeoesMb"
      },
      "source": [
        "## Why train error is much smaller than test?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kj0_7ZZFgAya"
      },
      "source": [
        "# Train predictions\n",
        "pics = <YOUR CODE> # reconstruct and glue together X and predicted Y for the train dataset\n",
        "\n",
        "plt.figure(figsize=[16, 12])\n",
        "for i in range(20):\n",
        "    plt.subplot(4, 5, i + 1)\n",
        "    plt.imshow(pics[i], cmap='gray')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MCiLbOTugSV8"
      },
      "source": [
        "# Test predictions\n",
        "pics = <YOUR CODE> # reconstruct and glue together X and predicted Y for the test dataset\n",
        "\n",
        "plt.figure(figsize=[16, 12])\n",
        "for i in range(20):\n",
        "    plt.subplot(4, 5, i + 1)\n",
        "    plt.imshow(pics[i], cmap='gray')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rZZF6Ns0gsIm"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mPtPdBCMesMm"
      },
      "source": [
        "Remember regularisation? That is exactly what we need. There are many many linear models in sklearn package, and all of them can be found [here](https://scikit-learn.org/stable/modules/linear_model.html). We will focus on 3 of them: Ridge regression, Lasso and ElasticNet.\n",
        "Idea of all of them is very simple: Add some penalty to the objective loss function to prevent overfitting."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ucmL5LNSesMm"
      },
      "source": [
        "# Ridge regression\n",
        "RidgeRegression is just a LinearRegression, with l2 regularization - penalized for $ \\alpha \\cdot \\sum _i w_i^2$\n",
        "\n",
        "Let's train such a model with alpha=0.5"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8IxtafdhesMo"
      },
      "source": [
        "from <YOUR CODE> import <YOUR CODE>\n",
        "\n",
        "ridge = <YOUR CODE>\n",
        "\n",
        "<YOUR CODE: fit the model on training set>"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C0UAnpxQesMr"
      },
      "source": [
        "<YOUR CODE: predict and measure MSE on train and test>"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZgOK0gfFiirK"
      },
      "source": [
        "# Test predictions\n",
        "pics = <YOUR CODE> # reconstruct and glue together X and predicted Y for the test dataset\n",
        "\n",
        "plt.figure(figsize=[16, 12])\n",
        "for i in range(20):\n",
        "    plt.subplot(4, 5, i + 1)\n",
        "    plt.imshow(pics[i], cmap='gray')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z0xpbcZVesMy"
      },
      "source": [
        "```\n",
        "\n",
        "```\n",
        "\n",
        "```\n",
        "\n",
        "```\n",
        "\n",
        "```\n",
        "\n",
        "```\n",
        "\n",
        "```\n",
        "\n",
        "```\n",
        "\n",
        "```\n",
        "\n",
        "```\n",
        "\n",
        "```\n",
        "\n",
        "```\n",
        "\n",
        "```\n",
        "\n",
        "```\n",
        "\n",
        "```\n",
        "\n",
        "```\n",
        "\n",
        "# Finding the best `alpha` (grid search)\n",
        "\n",
        "`sklearn` has a pre-implemented class - `sklearn.model_selection.GridSearchCV` - that wraps your model and optimizes its hyperparameters using K-fold cross-validation. The hyperparameter values are taken from a finite set of values in a rectangular grid (therefore, the method is called grid search). In order to use it, you need to set the hyperparameter values grid, the metric you want to optimize and the number of folds from the cross-validation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qgy_LgwNesMz"
      },
      "source": [
        "from sklearn.model_selection import GridSearchCV"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L8ETgdXIesM1"
      },
      "source": [
        "parameter_dict = {\n",
        "    \"alpha\" : np.logspace(-3, 3, 13, base=10)\n",
        "}\n",
        "\n",
        "gscv = GridSearchCV(\n",
        "    estimator=Ridge(), # our model to optimize\n",
        "    param_grid=parameter_dict, # grid of parameter values\n",
        "    scoring='neg_mean_squared_error', # metric - it needs to be a score, so\n",
        "                                      # we take the negative MSE\n",
        "    cv=5, verbose=2, n_jobs=-1\n",
        ")\n",
        "gscv.fit(X_train, Y_train)\n",
        "\n",
        "plt.errorbar(gscv.param_grid['alpha'],\n",
        "             gscv.cv_results_['mean_test_score'],\n",
        "             gscv.cv_results_['std_test_score'] / gscv.cv**0.5,\n",
        "             capsize=5)\n",
        "plt.xscale(\"log\", nonposx='clip')\n",
        "plt.xlabel(\"alpha\")\n",
        "plt.ylabel(\"negative MSE\")\n",
        "plt.grid()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LBeswJPABJpQ"
      },
      "source": [
        "Now you can find the best model as `gscv.best_estimator_`. Use it to make the reconstruction on the test again:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QQGIIuhbesNA"
      },
      "source": [
        "# Test predictions\n",
        "pics = glue(X_test, <predict with your best model>)\n",
        "plt.figure(figsize=[16, 12])\n",
        "for i in range(20):\n",
        "    plt.subplot(4, 5, i + 1)\n",
        "    plt.imshow(pics[i], cmap='gray')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "433Mv13AesNG"
      },
      "source": [
        "from sklearn.linear_model import Lasso, ElasticNet\n",
        "\n",
        "# Use the code above to do GridSearch for Lasso and/or ElasticNet\n",
        "# models. Note that Lasso and ElasticNet are *much*\n",
        "# slower to fit, compared to Ridge (especially for small alpha).\n",
        "<YOUR CODE>"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TVe4ROnLFAg_"
      },
      "source": [
        "# Side note: sklearn transformers and pipelines"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U9czWvlFFFmN"
      },
      "source": [
        "It's typical to pre-process the data before plugging it into the machine learning models. Although this can be done by hand, `sklearn` implements a common transformer interface:\n",
        "```python\n",
        "some_transformer = sklearn.some_module.SomeTransformerClass(some_parameters) # create the transformer object\n",
        "some_transformer.fit(X, y) # learn how to transform the data (e.g.\n",
        "                           # for StandardScaler, calculate mean and std of columns in X)\n",
        "some_transformer.transform(X) # transform the features\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qbHpeYB3Hbm7"
      },
      "source": [
        "`StandardScaler` example:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0nLC9az4FDgE"
      },
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "scaler = StandardScaler()\n",
        "scaler.fit(X_train) # y is optional, not used in StandardScaler\n",
        "\n",
        "X_train_transformed = scaler.transform(X_train)\n",
        "X_test_transformed = scaler.transform(X_test)\n",
        "\n",
        "print(X_train_transformed.mean(axis=0))\n",
        "print(X_train_transformed.std(axis=0))\n",
        "print(X_test_transformed.mean(axis=0))\n",
        "print(X_test_transformed.std(axis=0))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pr0sFXUcI8wK"
      },
      "source": [
        "In the above code, though using a sklearn preprocessing class, we are still transforming the data by hand in a sense - i.e. we are creating `X_train_transformed` and `X_test_transformed` that later would be used by our algorithm.\n",
        "\n",
        "However, there is a neat way of incorporating any set of such transformations into our model directly. This is called a pipeline:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ks4t0aSnJazW"
      },
      "source": [
        "from sklearn.pipeline import make_pipeline\n",
        "\n",
        "model = make_pipeline(\n",
        "    StandardScaler(),\n",
        "    Ridge(alpha=10.)\n",
        ")\n",
        "model.fit(X_train, Y_train) # at this step the model:\n",
        "                            #   - first fits the standard scaler\n",
        "                            #   - then transforms X_train with it\n",
        "                            #   - then fits the ridge regression with\n",
        "                            #     the transformed X_train\n",
        "\n",
        "print(\"Train MSE:\", mean_squared_error(Y_train, model.predict(X_train)))\n",
        "print(\"Test MSE:\", mean_squared_error(Y_test, model.predict(X_test)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n4Ww237DKTQX"
      },
      "source": [
        "Such pipelines can be used within grid search CV, e.g.:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mpu7hE-GKLy3"
      },
      "source": [
        "parameter_dict = {\n",
        "    \"ridge__alpha\" : np.logspace(-1, 7, 9, base=10) # note the 'ridge__' prefix\n",
        "                                                    # that tells to which step\n",
        "                                                    # of the pipeline this\n",
        "                                                    # parameter belongs\n",
        "}\n",
        "\n",
        "gscv = GridSearchCV(\n",
        "    estimator=model,\n",
        "    param_grid=parameter_dict,\n",
        "    scoring='neg_mean_squared_error',\n",
        "    cv=5, verbose=2, n_jobs=-1\n",
        ")\n",
        "gscv.fit(X_train, Y_train)\n",
        "\n",
        "\n",
        "plt.errorbar(gscv.param_grid['ridge__alpha'],\n",
        "             gscv.cv_results_['mean_test_score'],\n",
        "             gscv.cv_results_['std_test_score'] / gscv.cv**0.5,\n",
        "             capsize=5)\n",
        "plt.xscale(\"log\", nonposx='clip')\n",
        "plt.xlabel(\"alpha\")\n",
        "plt.ylabel(\"negative MSE\")\n",
        "plt.grid()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WNncsNN4IWh9"
      },
      "source": [
        "See also:\n",
        "- [sklearn.preprocessing.MinMaxScaler](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html) - scaling based on min and max values of the feature\n",
        "- [sklearn.preprocessing.RobustScaler](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.RobustScaler.html) - scaling that ignores the tails of the feature distribution (more robust to outliers)\n",
        "- [sklearn.preprocessing.OneHotEncoder](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html) - one-hot encoding for categorical features\n",
        "- [sklearn.preprocessing.FunctionTransformer](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.FunctionTransformer.html) - transformation using arbitrary (e.g. user defined) python function\n",
        "- [sklearn.compose.ColumnTransformer](https://scikit-learn.org/stable/modules/generated/sklearn.compose.ColumnTransformer.html) - tool that allows composing complicated nested structures of transformers (e.g. for using different transformers for different features)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IK8MqkJ4esNJ"
      },
      "source": [
        "# Outliers impact on regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wN4QiVwKesNK"
      },
      "source": [
        "Remember, that when we minimise loss\n",
        "$$\n",
        "MSE = (\\hat y - y)^2\n",
        "$$\n",
        "\n",
        "which penalise more to for heigher values of error. What is that impact of this for the dataset with outliers, what do you think?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FP4DRSJ_esNL"
      },
      "source": [
        "Here is an example of regression fitted with ordinary LR and RANSAC, which iteratively trains on random subsamples of the data, trying to identify outliers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XwDJZxuBesNM"
      },
      "source": [
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "from sklearn import linear_model, datasets\n",
        "\n",
        "\n",
        "n_samples = 1000\n",
        "n_outliers = 50\n",
        "\n",
        "\n",
        "X, y, coef = datasets.make_regression(n_samples=n_samples, n_features=1,\n",
        "                                      n_informative=1, noise=10,\n",
        "                                      coef=True, random_state=0)\n",
        "\n",
        "# Add outlier data\n",
        "np.random.seed(0)\n",
        "X[:n_outliers] = 3 + 0.5 * np.random.normal(size=(n_outliers, 1))\n",
        "y[:n_outliers] = -3 + 10 * np.random.normal(size=n_outliers)\n",
        "\n",
        "# Fit line using all data\n",
        "lr = linear_model.LinearRegression()\n",
        "lr.fit(X, y)\n",
        "\n",
        "# Robustly fit linear model with RANSAC algorithm\n",
        "ransac = linear_model.RANSACRegressor()\n",
        "ransac.fit(X, y)\n",
        "inlier_mask = ransac.inlier_mask_\n",
        "outlier_mask = np.logical_not(inlier_mask)\n",
        "\n",
        "# Predict data of estimated models\n",
        "line_X = np.arange(X.min(), X.max())[:, np.newaxis]\n",
        "line_y = lr.predict(line_X)\n",
        "line_y_ransac = ransac.predict(line_X)\n",
        "\n",
        "# Compare estimated coefficients\n",
        "print(\"Estimated coefficients (true, linear regression, RANSAC):\")\n",
        "print(coef, lr.coef_, ransac.estimator_.coef_)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ISdoPPi3esNQ"
      },
      "source": [
        "plt.figure(figsize=(12,6))\n",
        "lw = 2\n",
        "plt.scatter(X[inlier_mask], y[inlier_mask], color='yellowgreen', marker='.',\n",
        "            label='Inliers')\n",
        "plt.scatter(X[outlier_mask], y[outlier_mask], color='gold', marker='.',\n",
        "            label='Outliers')\n",
        "plt.plot(line_X, line_y, color='navy', linewidth=lw, label='Linear regressor')\n",
        "plt.plot(line_X, line_y_ransac, color='cornflowerblue', linewidth=lw,\n",
        "         label='RANSAC regressor')\n",
        "plt.legend(loc='lower right')\n",
        "plt.xlabel(\"Input\")\n",
        "plt.ylabel(\"Response\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oJkJpSK4qNzl"
      },
      "source": [
        "## Bonus part"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ZBAqmrrqQch"
      },
      "source": [
        "Try using `sklearn.linear_model.SGDRegressor` with `huber` loss in the code above instead of `LinearRegression`. Is it better in this case? Try varying its `epsilon` parameter."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xa1S9pwWesNT"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}